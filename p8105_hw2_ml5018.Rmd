---
title: "p8105_hw2_ml5018"
author: "Luan Mengxiao"
date: 2023-09-26
output: github_document
---

This is a R Markdown document for homework 2.

Load the package to be used for data processing.

```{r setup, message = FALSE}
library(tidyverse)
options(tibble.print_min = 5)
```

# Problem 1

## pols_month

First, clean the data in pols-month.csv. Use `separate()` to break up the variable mon into integer variables `year`, `month`, and `day`; replace month number with month name; create a `president` variable taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; and remove the `day` variable.

```{r pols_month}
pols_month_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)]) |>
  mutate(president = prez_gop - prez_dem,
         president = case_match(president, 
                                2 ~ "gop", 1  ~ "gop", -1 ~ "dem")) |>
  select(-prez_dem, -prez_gop,-day)

pols_month_df
```

## snp

Second, clean the data in snp.csv using a similar process to the above. For consistency across datasets, arrange according to year and month, and organize so that `year` and `month` are the leading columns.

```{r snp}
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year),
         year = as.character(case_when(year >= 23 ~ year + 1900,
                          year <  23 ~ year + 2000))) |>
  arrange(year, month)

snp_df
```

Considering that the `close` variable stands for the closing value on the associate date, it might be better if we reserve the variable `day` to keep the dataset intact and explicit.

Or, if we arrange the date first and then separate it into year, month and day, using the following code chunk we get the very same data frame as above:

```{r alternative, eval = FALSE}
snp_df_alter = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  mutate(date = as.Date(date, format = "%m/%d/%y"),
         date = ifelse(date > Sys.Date(),
                       format(date, "19%y-%m-%d"),
                       format(date))) |>
  separate(date, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year)) |>
  arrange(year, month)
```

## unemployment

Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.

```{r unemployment}
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(jan:dec, 
               names_to = "month", 
               values_to = "percentage_of_unemployment") |>
  mutate(year = as.character(year),
         month = recode(month,
                        jan = "January",
                        feb = "February",
                        mar = "March",
                        apr = "April",
                        may = "May",
                        jun = "June",
                        jul = "July",
                        aug = "August",
                        sep = "September",
                        oct = "October",
                        nov = "November",
                        dec = "December"
                        )
         )

unemployment_df
```

## join datasets

Join the datasets by merging `snp` into `pols`, and merging `unemployment` into the result.

```{r join}
problem1_df = 
  left_join(pols_month_df, snp_df) |>
  left_join(unemployment_df)

problem1_df
```

Since there are no corresponding variables to it in the other two datasets, we may also remove the `day` variable from `snp_df` before merging it, depending on the usage of the results, as shown in the code chunk below:

```{r remove_join, eval = FALSE}
snp_df_rm = select(snp_df, -day)

problem1_df_rm = 
  left_join(pols_month_df, snp_df_rm) |>
  left_join(unemployment_df)

problem_df_rm
```

## describe datasets

Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables).

The main data contained in the three datasets are listed as follow:

The file “pols-month” contains 822 observations of 9 variables related to the number of national politicians who are democratic or republican at any given time:

* mon: date of the count
* prez_gop: indicator of whether the president was republican on the associated date (1 = yes, 0 = no)
* gov_gop: the number of republican governors on the associated date
* sen_gop: the number of republican senators on the associated date
* rep_gop: the number of republican representatives on the associated date
* prez_dem: indicator of whether the president was democratic on the associated date (1 = yes, 0 = no)
* gov_dem: the number of democratic governors on the associated date
* sen_dem: the number of democratic senators on the associated date
* rep_dem: the number of democratic representatives on the associated date

The file “snp” contains 787 observations of 2 variables related to Standard & Poor’s stock market index (S&P), often used as a representative measure of stock market as a whole:

* date: the date of the observation
* close: the closing values of the S&P stock index on the associated date

The file “unemployment” contains 68 observations of 13 variables:

* Year: the year of the measurements on that row
* Jan: percentage of unemployment in January of the associated year
* Feb: percentage of unemployment in February of the associated year
* Mar: percentage of unemployment in March of the associated year
* Apr: percentage of unemployment in April of the associated year
* May: percentage of unemployment in May of the associated year
* Jun: percentage of unemployment in June of the associated year
* Jul: percentage of unemployment in July of the associated year
* Aug: percentage of unemployment in August of the associated year
* Sep: percentage of unemployment in September of the associated year
* Oct: percentage of unemployment in October of the associated year
* Nov: percentage of unemployment in November of the associated year
* Dec: percentage of unemployment in December of the associated year

Using the code chunk below to further describe the resulting dataset:

```{r describe_1}
skimr::skim(problem1_df)
```

The merged dataset contains `r dim(problem1_df)[1]` observations of `r dim(problem1_df)[2]` variables.

The variable `year` included in this dataset ranges from `r range(pull(problem1_df, var = 1))[1]` to `r range(pull(problem1_df, var = 1))[2]`.

The names of some key variables in the dataset include: `r names(problem1_df)`.

# Problem 2

## import and tidy Mr. Trash Wheel

Read and clean the Mr. Trash Wheel sheet:

* specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in `read_excel`
* use reasonable variable names
* omit rows that do not include dumpster-specific data

```{r mr_wheel_trash}
mr_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Mr. Trash Wheel",
                     range = "A2:N587") |>
  janitor::clean_names() |>
  drop_na(dumpster)

mr_wheel_trash_df
```

## update Mr. Trash Wheel

The data include a column for the (approximate) number of homes powered. This calculation is described in the `Homes powered note`, but not applied to every row in the dataset. Update the data to include a new `homes_powered` variable based on this calculation.

* Homes Powered - Each ton of trash equates to on average 500 kilowatts of electricity.  An average household will use 30 kilowatts per day.

```{r update}
mr_wheel_trash_df = mutate(mr_wheel_trash_df,
                           homes_powered = weight_tons * 500 / 30)

mr_wheel_trash_df
```

## tidy and process more data

Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to all datasets before combining.

```{r more_data}
mr_wheel_trash_df = mutate(mr_wheel_trash_df, trash_wheel = "mr")

mr_wheel_trash_df

professor_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Professor Trash Wheel",
                     range = "A2:M109") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "professor",
         year = as.character(year))

professor_wheel_trash_df

gwynnda_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Gwynnda Trash Wheel",
                     range = "A2:L159") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "gwynnda",
         year = as.character(year))

gwynnda_df

problem2_df = 
  full_join(mr_wheel_trash_df, professor_wheel_trash_df) |>
  full_join(gwynnda_df)

problem2_df
```

## describe the dataset

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in July of 2021?

Mr. Trash Wheel is a water-wheel vessel that removes trash from the Inner Harbor in Baltimore, Maryland. The combined datasets include the information related to the date, weight and sort of trash collected by Mr. , Professor and Gwynnda Trash Wheels.

Apply the code below to take a brief view at the dataset.

```{r describe_2}
skimr::skim(problem2_df)
```

The resulting dataset consists of `r nrow(problem2_df)` observations of `r ncol(problem2_df)` variables.

The key variables in the dataset include: `r names(problem2_df)`.

For available data, the total weight of trash collected by Professor Trash Wheel is `r sum(pull(filter(problem2_df, trash_wheel == "professor"),5))` tons.

And the total number of cigarette butts collected by Gwynnda in July of 2021 is `r sum(pull(filter(problem2_df, year == 2021, month == "July", trash_wheel == "gwynnda"), 9))`.

# Problem 3

## import and tidy baseline data

Import, clean, and tidy the dataset of baseline demographics. Ensure that sex and APOE4 carrier status are appropriate encoded (i.e. not numeric), and remove any participants who do not meet the stated inclusion criteria (i.e. no MCI at baseline). Discuss important steps in the import process and relevant features of the dataset. How many participants were recruited, and of these how many develop MCI? What is the average baseline age? What proportion of women in the study are APOE4 carriers?

```{r baseline}
mci_baseline_df = 
  read_csv("data/data_mci/MCI_baseline.csv", 
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor()),
           na = c(".")
           ) |>
  janitor::clean_names() |>
  filter(current_age <= age_at_onset | is.na(age_at_onset))

mci_baseline_df
```

The data was collected in an observational study to understand the trajectory of Alzheimer’s disease (AD) biomarkers. Study participants were free of Mild Cognitive Impairment (MCI), a stage between the expected cognitive decline of normal aging and the more serious decline of dementia, at the study baseline.

Basic demographic information were measured at the study baseline. The study monitored the development of MCI and recorded the age of MCI onset during the follow-up period, with the last visit marking the end of follow-up. APOE4 is a variant of the apolipoprotein E gene, significantly associated with a higher risk of developing Alzheimer’s disease.

Some important steps in the process include skipping the first row which contains no variavle names or values but notes, setting the variable type of some specific columns, and converting missing values to `NA` before dropping the rows containing them.

The original dataset consists of `r nrow(read_csv("data/data_mci/MCI_baseline.csv"))` observations of `r ncol(read_csv("data/data_mci/MCI_baseline.csv"))` variables. Some key variables include: `r names(read_csv("data/data_mci/MCI_baseline.csv"))`.

After a primary filtering, the resulting dataset is composed by `r nrow(mci_baseline_df)` rows and `r ncol(mci_baseline_df)` columns.

It can be concluded that `r nrow(read_csv("data/data_mci/MCI_baseline.csv"))` participants were recruited at the beginning of the study, and `r nrow(mci_baseline_df)` of them developed MCI during the track.

Use the following code chunk to calculate the average baseline age and proportion of APOE4 carriers in women.

```{r calculate}
baseline_origin_df = 
  read_csv("data/data_mci/MCI_baseline.csv",
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor())) |>
  janitor::clean_names()
average_age = mean(pull(baseline_origin_df,var = 2))
baseline_women_df = filter(baseline_origin_df, sex == 0)
baseline_women_carrier_df = filter(baseline_women_df, apoe4 == 1)
carrier_proportion = 
  as.numeric((count(baseline_women_carrier_df) / count(baseline_women_df))[1])
```

Thus the average baseline age of the study is `r average_age` years and `r carrier_proportion * 100`% proportion of the women are APOE4 carriers.

```{r filtered}
filtered_women_df = filter(mci_baseline_df, sex == 0)
filtered_women_carrier_df = filter(filtered_women_df, apoe4 == 1)
carrier_proportion_filtered = 
  as.numeric((count(filtered_women_carrier_df) / count(filtered_women_df))[1])
```

As shown above, if we exclude the subjects that did not meet the criteria for baseline study, then the average baseline age will be `r mean(pull(mci_baseline_df, var = 2))` years and the proportion of APOE4 carriers in women will be `r carrier_proportion_filtered * 100`%.

## import and process other data

Similarly, import, clean, and tidy the dataset of longitudinally observed biomarker values; comment on the steps on the import process and the features of the dataset.

```{r other_data}
mci_amyloid_df = 
  read_csv("data/data_mci/mci_amyloid.csv", 
           skip = 1
           ) |>
  janitor::clean_names() |>
  rename(id = study_id)

mci_amyloid_df
```

The amyloid β 42/40 ratio holds significant promise for diagnosing and predicting disease outcomes. This ratio undergoes changes over time and has been linked to the manifestation of clinical symptoms of Alzheimer’s disease.

The main step in import process is to skip the first row to avoid notes. And for future merging of datasets, convert the variable name to the same.

The dataset contains `r nrow(mci_amyloid_df)` rows and `r ncol(mci_amyloid_df)` columns, the main variables of which include: `r names(mci_amyloid_df)`.

If required, we can also use `pivot_longer` function to further tidy the dataset into a longer form, the code chunk to achieve which shown as follow:

```{r pivot}
mci_amyloid_longer_df = 
  mci_amyloid_df |> 
    pivot_longer(
    time_2:time_8,
    names_to = "time",
    names_prefix = "time_",
    values_to = "ratio"
  )

mci_amyloid_longer_df
```

## compare and join datasets

Check whether some participants appear in only the baseline or amyloid datasets, and comment on your findings. Combine the demographic and biomarker datasets so that only participants who appear in both datasets are retained, and briefly describe the resulting dataset; export the result as a CSV to your data directory.

Apply `anti_join` function to see the rows contained only in baseline dataset.

```{r antijoin1}
antijoin1_origin_df = anti_join(baseline_origin_df, mci_amyloid_df)

antijoin1_origin_df

antijoin1_df = anti_join(mci_baseline_df, mci_amyloid_df)

antijoin1_df

antijoin1_longer_df = anti_join(baseline_origin_df, mci_amyloid_longer_df)

antijoin1_longer_df
```

Similarly, use the code below for the rows contained only in amyloid dataset.

```{r antijoin2}
antijoin2_origin_df = anti_join(mci_amyloid_df, baseline_origin_df)

antijoin2_origin_df

antijoin2_df = anti_join(mci_amyloid_df, mci_baseline_df)

antijoin2_df

antijoin2_longer_df = anti_join(mci_amyloid_longer_df, baseline_origin_df)

antijoin2_longer_df
```

It is obvious that there exist some participants that only appear in the basline dataset or the amyloid dataset, with both of the comparisons returning a tibble, no matter the baseline dataset to be used has been filtered or not.

Comparing the results of `anti_join` function, it can be seen that there are much more observations that only appear in the amyloid dataset, especially after filtering the baseline dataset with the inclusion criteria. When using baseline dataset as the first input for `anti_join`, the resulting data frames were the same na matter the form of amyloid dataset was longer or wider. Yet when using amyloid as the first input, the resulting data frames differed due to the different forms of amyloid.

Use the function `inner_join` to combine the two datasets so that only participants who appear in both datasets will be retained. Then export the result and save it as a csv document.

```{r innerjoin}
innerjoin_df = inner_join(mci_baseline_df, mci_amyloid_df)

innerjoin_df

write_csv(innerjoin_df, "mci_combined.csv")
```

The resulting dataset consists of `r nrow(innerjoin_df)` observations of `r ncol(innerjoin_df)` variables. The key variables include `r names(innerjoin_df)`.

Further descriptive information can be obtained using the code chunk below.

```{r describe_3}
skimr::skim(innerjoin_df)
```

If we use the longer form of amyloid to do the process above:

```{r longer}
innerjoin_longer_df = inner_join(mci_baseline_df, mci_amyloid_longer_df)

innerjoin_longer_df

write_csv(innerjoin_longer_df, "mci_combined_longer.csv")

skimr::skim(innerjoin_longer_df)
```

The resulting dataset consists of `r nrow(innerjoin_longer_df)` observations of `r ncol(innerjoin_longer_df)` variables. The key variables include `r names(innerjoin_longer_df)`.