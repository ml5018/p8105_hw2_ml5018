p8105_hw2_ml5018
================
Luan Mengxiao
2023-09-26

This is a R Markdown document for homework 2.

Load the package to be used for data processing.

``` r
library(tidyverse)
options(tibble.print_min = 5)
```

# Problem 1

## pols_month

First, clean the data in pols-month.csv. Use `separate()` to break up
the variable mon into integer variables `year`, `month`, and `day`;
replace month number with month name; create a `president` variable
taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; and
remove the `day` variable.

``` r
pols_month_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)]) |>
  mutate(president = prez_gop - prez_dem,
         president = case_match(president, 1  ~ "gop", -1 ~ "dem")) |>
  select(-prez_dem, -prez_gop,-day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
pols_month_df
```

    ## # A tibble: 822 × 9
    ##   year  month    gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ## 1 1947  January       23      51     253      23      45     198 dem      
    ## 2 1947  February      23      51     253      23      45     198 dem      
    ## 3 1947  March         23      51     253      23      45     198 dem      
    ## 4 1947  April         23      51     253      23      45     198 dem      
    ## 5 1947  May           23      51     253      23      45     198 dem      
    ## # ℹ 817 more rows

## snp

Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that `year` and `month` are the leading columns.

``` r
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year),
         year = as.character(case_when(year >= 23 ~ year + 1900,
                          year <  23 ~ year + 2000))) |>
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_df
```

    ## # A tibble: 787 × 4
    ##   month    day   year  close
    ##   <chr>    <chr> <chr> <dbl>
    ## 1 April    3     1950   18.0
    ## 2 August   1     1950   18.4
    ## 3 December 1     1950   20.4
    ## 4 February 1     1950   17.2
    ## 5 January  3     1950   17.0
    ## # ℹ 782 more rows

## unemployment

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

``` r
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(jan:dec, 
               names_to = "month", 
               values_to = "percentage_of_unemployment") |>
  mutate(year = as.character(year),
         month = recode(month,
                        jan = "January",
                        feb = "February",
                        mar = "March",
                        apr = "April",
                        may = "May",
                        jun = "June",
                        jul = "July",
                        aug = "August",
                        sep = "September",
                        oct = "October",
                        nov = "November",
                        dec = "December"
                        )
         )
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemployment_df
```

    ## # A tibble: 816 × 3
    ##   year  month    percentage_of_unemployment
    ##   <chr> <chr>                         <dbl>
    ## 1 1948  January                         3.4
    ## 2 1948  February                        3.8
    ## 3 1948  March                           4  
    ## 4 1948  April                           3.9
    ## 5 1948  May                             3.5
    ## # ℹ 811 more rows

## join datasets

Join the datasets by merging `snp` into `pols`, and merging
`unemployment` into the result.

``` r
problem1_df = 
  left_join(pols_month_df, snp_df) |>
  left_join(unemployment_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

``` r
problem1_df
```

    ## # A tibble: 822 × 12
    ##   year  month    gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president day  
    ##   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>     <chr>
    ## 1 1947  January       23      51     253      23      45     198 dem       <NA> 
    ## 2 1947  February      23      51     253      23      45     198 dem       <NA> 
    ## 3 1947  March         23      51     253      23      45     198 dem       <NA> 
    ## 4 1947  April         23      51     253      23      45     198 dem       <NA> 
    ## 5 1947  May           23      51     253      23      45     198 dem       <NA> 
    ## # ℹ 817 more rows
    ## # ℹ 2 more variables: close <dbl>, percentage_of_unemployment <dbl>

## describe datasets

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

The main data contained in the three datasets are listed as follow:

The file “pols-month” contains 822 observations of 9 variables related
to the number of national politicians who are democratic or republican
at any given time:

- mon: date of the count
- prez_gop: indicator of whether the president was republican on the
  associated date (1 = yes, 0 = no)
- gov_gop: the number of republican governors on the associated date
- sen_gop: the number of republican senators on the associated date
- rep_gop: the number of republican representatives on the associated
  date
- prez_dem: indicator of whether the president was democratic on the
  associated date (1 = yes, 0 = no)
- gov_dem: the number of democratic governors on the associated date
- sen_dem: the number of democratic senators on the associated date
- rep_dem: the number of democratic representatives on the associated
  date

The file “snp” contains 787 observations of 2 variables related to
Standard & Poor’s stock market index (S&P), often used as a
representative measure of stock market as a whole:

- date: the date of the observation
- close: the closing values of the S&P stock index on the associated
  date

The file “unemployment” contains 68 observations of 13 variables:

- Year: the year of the measurements on that row
- Jan: percentage of unemployment in January of the associated year
- Feb: percentage of unemployment in February of the associated year
- Mar: percentage of unemployment in March of the associated year
- Apr: percentage of unemployment in April of the associated year
- May: percentage of unemployment in May of the associated year
- Jun: percentage of unemployment in June of the associated year
- Jul: percentage of unemployment in July of the associated year
- Aug: percentage of unemployment in August of the associated year
- Sep: percentage of unemployment in September of the associated year
- Oct: percentage of unemployment in October of the associated year
- Nov: percentage of unemployment in November of the associated year
- Dec: percentage of unemployment in December of the associated year

Using the code chunk below to further describe the resulting dataset:

``` r
skimr::skim(problem1_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem1_df |
| Number of rows                                   | 822         |
| Number of columns                                | 12          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 4           |
| numeric                                          | 8           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| year          |         0 |          1.00 |   4 |   4 |     0 |       69 |          0 |
| month         |         0 |          1.00 |   3 |   9 |     0 |       12 |          0 |
| president     |         5 |          0.99 |   3 |   3 |     0 |        2 |          0 |
| day           |        36 |          0.96 |   1 |   1 |     0 |        4 |          0 |

**Variable type: numeric**

| skim_variable              | n_missing | complete_rate |   mean |     sd |     p0 |    p25 |    p50 |    p75 |    p100 | hist  |
|:---------------------------|----------:|--------------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|:------|
| gov_gop                    |         0 |          1.00 |  22.48 |   5.68 |  12.00 |  18.00 |  22.00 |  28.00 |   34.00 | ▆▆▇▅▅ |
| sen_gop                    |         0 |          1.00 |  46.10 |   6.38 |  32.00 |  42.00 |  46.00 |  51.00 |   56.00 | ▃▃▇▇▇ |
| rep_gop                    |         0 |          1.00 | 194.92 |  29.24 | 141.00 | 176.00 | 195.00 | 222.00 |  253.00 | ▃▇▆▃▅ |
| gov_dem                    |         0 |          1.00 |  27.20 |   5.94 |  17.00 |  22.00 |  28.00 |  32.00 |   41.00 | ▆▅▇▆▂ |
| sen_dem                    |         0 |          1.00 |  54.41 |   7.37 |  44.00 |  48.00 |  53.00 |  58.00 |   71.00 | ▇▆▇▃▂ |
| rep_dem                    |         0 |          1.00 | 244.97 |  31.37 | 188.00 | 211.00 | 250.00 | 268.00 |  301.00 | ▇▂▇▇▅ |
| close                      |        36 |          0.96 | 472.85 | 543.29 |  17.05 |  83.67 | 137.26 | 932.06 | 2107.39 | ▇▁▂▁▁ |
| percentage_of_unemployment |        12 |          0.99 |   5.83 |   1.65 |   2.50 |   4.70 |   5.60 |   6.90 |   10.80 | ▃▇▅▂▁ |

The merged dataset contains 822 observations of 12 variables.

The variable `year` included in this dataset ranges from 1947 to 2015.

The names of some key variables in the dataset include: year, month,
gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president, day,
close, percentage_of_unemployment.

# Problem 2

## import and tidy Mr. Trash Wheel

Read and clean the Mr. Trash Wheel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  `read_excel`
- use reasonable variable names
- omit rows that do not include dumpster-specific data

``` r
mr_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Mr. Trash Wheel",
                     range = "A2:N550") |>
  janitor::clean_names() |>
  drop_na(dumpster)

mr_wheel_trash_df
```

    ## # A tibble: 547 × 14
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 542 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, grocery_bags <dbl>,
    ## #   chip_bags <dbl>, sports_balls <dbl>, homes_powered <dbl>

## update Mr. Trash Wheel

The data include a column for the (approximate) number of homes powered.
This calculation is described in the `Homes powered note`, but not
applied to every row in the dataset. Update the data to include a new
`homes_powered` variable based on this calculation.

- Homes Powered - Each ton of trash equates to on average 500 kilowatts
  of electricity. An average household will use 30 kilowatts per day.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df,
                           homes_powered = weight_tons * 500 / 30)

mr_wheel_trash_df
```

    ## # A tibble: 547 × 14
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 542 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, grocery_bags <dbl>,
    ## #   chip_bags <dbl>, sports_balls <dbl>, homes_powered <dbl>

## tidy and process more data

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to all
datasets before combining.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df, trash_wheel = "mr")

mr_wheel_trash_df
```

    ## # A tibble: 547 × 15
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 542 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, grocery_bags <dbl>,
    ## #   chip_bags <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
professor_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Professor Trash Wheel",
                     range = "A2:M97") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "professor",
         year = as.character(year))

professor_wheel_trash_df
```

    ## # A tibble: 94 × 14
    ##   dumpster month    year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>    <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 January  2017  2017-01-02 00:00:00        1.79                 15
    ## 2        2 January  2017  2017-01-30 00:00:00        1.58                 15
    ## 3        3 February 2017  2017-02-26 00:00:00        2.32                 18
    ## 4        4 February 2017  2017-02-26 00:00:00        3.72                 15
    ## 5        5 February 2017  2017-02-28 00:00:00        1.45                 15
    ## # ℹ 89 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, grocery_bags <dbl>,
    ## #   chip_bags <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
gwynnda_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Gwynnda Trash Wheel",
                     range = "A2:K110") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "gwynnda",
         year = as.character(year))

gwynnda_df
```

    ## # A tibble: 106 × 12
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 July  2021  2021-07-03 00:00:00        0.93                 15
    ## 2        2 July  2021  2021-07-07 00:00:00        2.26                 15
    ## 3        3 July  2021  2021-07-07 00:00:00        1.62                 15
    ## 4        4 July  2021  2021-07-16 00:00:00        1.76                 15
    ## 5        5 July  2021  2021-07-30 00:00:00        1.53                 15
    ## # ℹ 101 more rows
    ## # ℹ 6 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, homes_powered <dbl>,
    ## #   trash_wheel <chr>

``` r
problem2_df = 
  full_join(mr_wheel_trash_df, professor_wheel_trash_df) |>
  full_join(gwynnda_df)
```

    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## glass_bottles, grocery_bags, chip_bags, homes_powered, trash_wheel)`
    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## homes_powered, trash_wheel)`

``` r
problem2_df
```

    ## # A tibble: 747 × 16
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 742 more rows
    ## # ℹ 10 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, grocery_bags <dbl>,
    ## #   chip_bags <dbl>, sports_balls <dbl>, homes_powered <dbl>,
    ## #   trash_wheel <chr>, plastic_bags <dbl>

## describe the dataset

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in July of 2021?

Apply the code below to take a brief view at the dataset.

``` r
skimr::skim(problem2_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem2_df |
| Number of rows                                   | 747         |
| Number of columns                                | 16          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 3           |
| numeric                                          | 12          |
| POSIXct                                          | 1           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| month         |         0 |             1 |   3 |   9 |     0 |       14 |          0 |
| year          |         0 |             1 |   4 |   4 |     0 |        9 |          0 |
| trash_wheel   |         0 |             1 |   2 |   9 |     0 |        3 |          0 |

**Variable type: numeric**

| skim_variable      | n_missing | complete_rate |     mean |       sd |     p0 |     p25 |     p50 |      p75 |      p100 | hist  |
|:-------------------|----------:|--------------:|---------:|---------:|-------:|--------:|--------:|---------:|----------:|:------|
| dumpster           |         0 |          1.00 |   214.10 |   168.33 |   1.00 |   62.50 |  174.00 |   360.50 |    547.00 | ▇▃▃▃▃ |
| weight_tons        |         0 |          1.00 |     3.01 |     0.83 |   0.61 |    2.49 |    3.07 |     3.57 |      5.62 | ▁▅▇▃▁ |
| volume_cubic_yards |         0 |          1.00 |    15.17 |     1.40 |   5.00 |   15.00 |   15.00 |    15.00 |     20.00 | ▁▁▁▇▁ |
| plastic_bottles    |         0 |          1.00 |  2290.85 |  1773.09 |   0.00 |  980.00 | 1930.00 |  2900.00 |   9830.00 | ▇▆▁▁▁ |
| polystyrene        |         0 |          1.00 |  1815.69 |  1962.61 |   0.00 |  410.00 | 1120.00 |  2625.00 |  11528.00 | ▇▂▁▁▁ |
| cigarette_butts    |         0 |          1.00 | 17183.95 | 26817.38 |   0.00 | 3450.00 | 6400.00 | 19000.00 | 310000.00 | ▇▁▁▁▁ |
| glass_bottles      |       106 |          0.86 |    20.71 |    15.82 |   0.00 |    9.00 |   18.00 |    28.00 |    110.00 | ▇▃▁▁▁ |
| grocery_bags       |       106 |          0.86 |  1217.66 |  1634.36 |  24.00 |  360.00 |  780.00 |  1480.00 |  13450.00 | ▇▁▁▁▁ |
| chip_bags          |       106 |          0.86 |  2405.54 |  3050.01 | 180.00 |  800.00 | 1340.00 |  2684.00 |  20100.00 | ▇▁▁▁▁ |
| sports_balls       |       200 |          0.73 |    12.58 |     9.27 |   0.00 |    6.00 |   11.00 |    18.00 |     56.00 | ▇▅▂▁▁ |
| homes_powered      |         0 |          1.00 |    50.18 |    13.77 |  10.17 |   41.42 |   51.17 |    59.50 |     93.67 | ▁▅▇▃▁ |
| plastic_bags       |       641 |          0.14 |   977.64 |   869.02 |   0.00 |  242.50 |  810.00 |  1475.00 |   3600.00 | ▇▃▂▁▁ |

**Variable type: POSIXct**

| skim_variable | n_missing | complete_rate | min        | max        | median     | n_unique |
|:--------------|----------:|--------------:|:-----------|:-----------|:-----------|---------:|
| date          |         0 |             1 | 1900-01-20 | 2022-07-29 | 2019-04-18 |      400 |

The resulting dataset consists of 747 observations of 16 variables.

The key variables in the dataset include: dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, grocery_bags, chip_bags, sports_balls,
homes_powered, trash_wheel, plastic_bags.

For available data, the total weight of trash collected by Professor
Trash Wheel is 190.12 tons.

And the total number of cigarette butts collected by Gwynnda in July of
2021 is 1.63^{4}.

# Problem 3

## import and tidy baseline data

Import, clean, and tidy the dataset of baseline demographics. Ensure
that sex and APOE4 carrier status are appropriate encoded (i.e. not
numeric), and remove any participants who do not meet the stated
inclusion criteria (i.e. no MCI at baseline). Discuss important steps in
the import process and relevant features of the dataset. How many
participants were recruited, and of these how many develop MCI? What is
the average baseline age? What proportion of women in the study are
APOE4 carriers?

``` r
mci_baseline_df = 
  read_csv("data/data_mci/MCI_baseline.csv", 
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor()),
           na = c(".")
           ) |>
  janitor::clean_names() |>
  drop_na(age_at_onset)

mci_baseline_df
```

    ## # A tibble: 97 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl>
    ## 1     3        62.5 1            16 1             66.8
    ## 2     5        66   1            16 0             68.7
    ## 3     7        66.5 1            18 0             74  
    ## 4    13        63.1 1            12 1             69  
    ## 5    14        58.4 0            20 0             66.2
    ## # ℹ 92 more rows

Some important steps in the process include skipping the first row which
contains no variavle names or values but notes, setting the variable
type of some specific columns, and converting missing values to `NA`
before dropping the rows containing them.

The original dataset consists of 484 observations of 6 variables. Some
key variables include: …1, Age at the study baseline, 1 = Male, 0 =
Female, Years of education, 1 = APOE4 carrier, 0 = APOE4 non-carrier,
Age at the onset of MCI; missing if a subject remains MCI free during
the follow-up period.

After a primary filtering, the resulting dataset is composed by 97 rows
and 6 columns.

It can be concluded that 484 participants were recruited at the
beginning of the study, and 97 of them developed MCI during the track.

Use the following code chunk to calculate the average baseline age and
proportion of APOE4 carriers in women.

``` r
baseline_origin_df = 
  read_csv("data/data_mci/MCI_baseline.csv",
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor())) |>
  janitor::clean_names()
average_age = mean(baseline_origin_df$current_age)
baseline_women_df = filter(baseline_origin_df, sex == 0)
baseline_women_carrier_df = filter(baseline_women_df, apoe4 == 1)
carrier_proportion = 
  as.numeric((count(baseline_women_carrier_df) / count(baseline_women_df))[1])
```

Thus the average baseline age of the study is 65.0467909 years and
29.8578199% proportion of the women are APOE4 carriers.

## import and process other data

Similarly, import, clean, and tidy the dataset of longitudinally
observed biomarker values; comment on the steps on the import process
and the features of the dataset.

``` r
mci_amyloid_df = 
  read_csv("data/data_mci/mci_amyloid.csv", 
           skip = 1
           ) |>
  janitor::clean_names() |>
  rename(id = study_id)
```

    ## Rows: 487 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Baseline, Time 2, Time 4, Time 6, Time 8
    ## dbl (1): Study ID
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
mci_amyloid_df
```

    ## # A tibble: 487 × 6
    ##      id baseline    time_2      time_4      time_6      time_8     
    ##   <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ## 1     1 0.1105487   <NA>        0.109325197 0.104756131 0.107257697
    ## 2     2 0.107481183 0.109157373 0.109457839 0.105729713 0.10661845 
    ## 3     3 0.106087034 0.108744509 0.106065035 <NA>        0.106152357
    ## 4     4 0.109251358 0.108699686 0.110540386 0.107476797 0.111212209
    ## 5     5 0.107950408 0.112273883 0.115139677 0.106606054 0.106052066
    ## # ℹ 482 more rows

The main step in import process is to skip the first row to avoid notes.
And for future merging of datasets, convert the variable name to the
same.

The dataset contains 487 rows and 6 columns, the main variables of which
include: id, baseline, time_2, time_4, time_6, time_8.

## compare and join datasets

Check whether some participants appear in only the baseline or amyloid
datasets, and comment on your findings. Combine the demographic and
biomarker datasets so that only participants who appear in both datasets
are retained, and briefly describe the resulting dataset; export the
result as a CSV to your data directory.

Apply `anti_join` function to see the rows contained only in baseline
dataset.

``` r
antijoin1_origin_df = anti_join(baseline_origin_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_origin_df
```

    ## # A tibble: 8 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct> <chr>       
    ## 1    14        58.4 0            20 0     66.2        
    ## 2    49        64.7 1            16 0     68.4        
    ## 3    92        68.6 0            20 0     .           
    ## 4   179        68.1 1            16 0     .           
    ## 5   268        61.4 0            18 1     67.5        
    ## 6   304        63.8 0            16 0     .           
    ## 7   389        59.3 0            16 0     .           
    ## 8   412        67   1            16 1     .

``` r
antijoin1_df = anti_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_df
```

    ## # A tibble: 3 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl>
    ## 1    14        58.4 0            20 0             66.2
    ## 2    49        64.7 1            16 0             68.4
    ## 3   268        61.4 0            18 1             67.5

Similarly, use the code below for the rows contained only in amyloid
dataset.

``` r
antijoin2_origin_df = anti_join(mci_amyloid_df, baseline_origin_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_origin_df
```

    ## # A tibble: 12 × 6
    ##       id baseline    time_2      time_4      time_6      time_8     
    ##    <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ##  1   484 0.11139422  0.110936838 0.109182887 0.110607585 0.107057538
    ##  2   485 0.106042813 0.105158363 0.107758828 0.107281321 0.106181816
    ##  3   486 0.109161071 0.114634379 <NA>        0.110035156 0.107234758
    ##  4   487 0.110821971 0.107791347 0.109855229 0.110951271 0.105861634
    ##  5   488 0.110418756 0.111994328 0.113132987 0.108902038 0.109449907
    ##  6   489 0.11477384  0.113322128 0.115109381 0.116004489 0.112260161
    ##  7   490 0.111762756 0.109627815 0.111492905 0.110104053 <NA>       
    ##  8   491 0.116934974 0.113763228 0.111358448 0.110509854 0.110541984
    ##  9   492 0.109757685 0.109912273 0.110672861 0.109064952 0.109161341
    ## 10   493 0.108357146 0.108161281 0.109491179 0.104448142 0.108636703
    ## 11   494 0.116669151 0.109711076 0.112133216 0.111399722 0.108836759
    ## 12   495 Na          0.105142354 0.108149625 0.105918659 0.102512562

``` r
antijoin2_df = anti_join(mci_amyloid_df, mci_baseline_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_df
```

    ## # A tibble: 393 × 6
    ##      id baseline    time_2      time_4      time_6      time_8     
    ##   <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ## 1     1 0.1105487   <NA>        0.109325197 0.104756131 0.107257697
    ## 2     2 0.107481183 0.109157373 0.109457839 0.105729713 0.10661845 
    ## 3     4 0.109251358 0.108699686 0.110540386 0.107476797 0.111212209
    ## 4     6 0.112426974 0.112853415 0.11143945  0.110279277 0.114982747
    ## 5     8 0.109563372 0.109470828 <NA>        0.108742168 0.110268552
    ## # ℹ 388 more rows

Comparing the results of `anti_join` function, it can be seen that there
are much more observations that only appear in the amyloid dataset,
especially after filtering the baseline dataset with the inclusion
criteria.

Use the function `inner_join` to combine the two datasets so that only
participants who appear in both datasets will be retained. Then export
the result and save it as a csv document.

``` r
innerjoin_df = inner_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
innerjoin_df
```

    ## # A tibble: 94 × 11
    ##      id current_age sex   education apoe4 age_at_onset baseline    time_2 time_4
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl> <chr>       <chr>  <chr> 
    ## 1     3        62.5 1            16 1             66.8 0.106087034 0.108… 0.106…
    ## 2     5        66   1            16 0             68.7 0.107950408 0.112… 0.115…
    ## 3     7        66.5 1            18 0             74   0.112246391 <NA>   0.104…
    ## 4    13        63.1 1            12 1             69   0.110300505 0.108… 0.108…
    ## 5    18        67.8 1            16 0             69.8 0.114137255 0.107… 0.110…
    ## # ℹ 89 more rows
    ## # ℹ 2 more variables: time_6 <chr>, time_8 <chr>

``` r
write_csv(innerjoin_df, "mci_combined.csv")
```

The resulting dataset consists of 94 observations of 11 variables. The
key variables include id, current_age, sex, education, apoe4,
age_at_onset, baseline, time_2, time_4, time_6, time_8.

Further descriptive information can be obtained using the code chunk
below.

``` r
skimr::skim(innerjoin_df)
```

|                                                  |              |
|:-------------------------------------------------|:-------------|
| Name                                             | innerjoin_df |
| Number of rows                                   | 94           |
| Number of columns                                | 11           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |              |
| Column type frequency:                           |              |
| character                                        | 5            |
| factor                                           | 2            |
| numeric                                          | 4            |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |              |
| Group variables                                  | None         |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| baseline      |         0 |          1.00 |  10 |  11 |     0 |       94 |          0 |
| time_2        |        13 |          0.86 |  10 |  11 |     0 |       81 |          0 |
| time_4        |         8 |          0.91 |   9 |  11 |     0 |       86 |          0 |
| time_6        |         8 |          0.91 |  10 |  11 |     0 |       86 |          0 |
| time_8        |         4 |          0.96 |  10 |  11 |     0 |       90 |          0 |

**Variable type: factor**

| skim_variable | n_missing | complete_rate | ordered | n_unique | top_counts   |
|:--------------|----------:|--------------:|:--------|---------:|:-------------|
| sex           |         0 |             1 | FALSE   |        2 | 1: 50, 0: 44 |
| apoe4         |         0 |             1 | FALSE   |        2 | 1: 60, 0: 34 |

**Variable type: numeric**

| skim_variable | n_missing | complete_rate |   mean |     sd |   p0 |   p25 |   p50 |    p75 |  p100 | hist  |
|:--------------|----------:|--------------:|-------:|-------:|-----:|------:|------:|-------:|------:|:------|
| id            |         0 |             1 | 245.62 | 144.07 |  3.0 | 93.25 | 281.5 | 365.75 | 471.0 | ▇▃▃▇▆ |
| current_age   |         0 |             1 |  65.74 |   2.89 | 58.1 | 63.70 |  66.0 |  67.70 |  71.6 | ▂▅▇▇▅ |
| education     |         0 |             1 |  16.49 |   1.75 | 12.0 | 16.00 |  16.0 |  18.00 |  20.0 | ▁▁▇▂▁ |
| age_at_onset  |         0 |             1 |  70.36 |   3.60 | 61.2 | 68.23 |  70.2 |  73.33 |  77.2 | ▂▃▇▆▅ |
