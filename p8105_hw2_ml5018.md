p8105_hw2_ml5018
================
Luan Mengxiao
2023-09-26

This is a R Markdown document for homework 2.

Load the package to be used for data processing.

``` r
library(tidyverse)
options(tibble.print_min = 5)
```

# Problem 1

## pols_month

First, clean the data in pols-month.csv. Use `separate()` to break up
the variable mon into integer variables `year`, `month`, and `day`;
replace month number with month name; create a `president` variable
taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; and
remove the `day` variable.

``` r
pols_month_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)]) |>
  mutate(president = prez_gop - prez_dem,
         president = case_match(president, 
                                2 ~ "gop", 1  ~ "gop", -1 ~ "dem")) |>
  select(-prez_dem, -prez_gop,-day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
pols_month_df
```

    ## # A tibble: 822 × 9
    ##   year  month    gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ## 1 1947  January       23      51     253      23      45     198 dem      
    ## 2 1947  February      23      51     253      23      45     198 dem      
    ## 3 1947  March         23      51     253      23      45     198 dem      
    ## 4 1947  April         23      51     253      23      45     198 dem      
    ## 5 1947  May           23      51     253      23      45     198 dem      
    ## # ℹ 817 more rows

## snp

Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that `year` and `month` are the leading columns.

``` r
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year),
         year = as.character(case_when(year >= 23 ~ year + 1900,
                          year <  23 ~ year + 2000))) |>
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_df
```

    ## # A tibble: 787 × 4
    ##   month    day   year  close
    ##   <chr>    <chr> <chr> <dbl>
    ## 1 April    3     1950   18.0
    ## 2 August   1     1950   18.4
    ## 3 December 1     1950   20.4
    ## 4 February 1     1950   17.2
    ## 5 January  3     1950   17.0
    ## # ℹ 782 more rows

Considering that the `close` variable stands for the closing value on
the associate date, it might be better if we reserve the variable `day`
to keep the dataset intact and explicit.

Or, if we arrange the date first and then separate it into year, month
and day, using the following code chunk we get the very same data frame
as above:

``` r
snp_df_alter = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  mutate(date = as.Date(date, format = "%m/%d/%y"),
         date = ifelse(date > Sys.Date(),
                       format(date, "19%y-%m-%d"),
                       format(date))) |>
  separate(date, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year)) |>
  arrange(year, month)
```

## unemployment

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

``` r
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(jan:dec, 
               names_to = "month", 
               values_to = "percentage_of_unemployment") |>
  mutate(year = as.character(year),
         month = recode(month,
                        jan = "January",
                        feb = "February",
                        mar = "March",
                        apr = "April",
                        may = "May",
                        jun = "June",
                        jul = "July",
                        aug = "August",
                        sep = "September",
                        oct = "October",
                        nov = "November",
                        dec = "December"
                        )
         )
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemployment_df
```

    ## # A tibble: 816 × 3
    ##   year  month    percentage_of_unemployment
    ##   <chr> <chr>                         <dbl>
    ## 1 1948  January                         3.4
    ## 2 1948  February                        3.8
    ## 3 1948  March                           4  
    ## 4 1948  April                           3.9
    ## 5 1948  May                             3.5
    ## # ℹ 811 more rows

## join datasets

Join the datasets by merging `snp` into `pols`, and merging
`unemployment` into the result.

``` r
problem1_df = 
  left_join(pols_month_df, snp_df) |>
  left_join(unemployment_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

``` r
problem1_df
```

    ## # A tibble: 822 × 12
    ##   year  month    gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president day  
    ##   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>     <chr>
    ## 1 1947  January       23      51     253      23      45     198 dem       <NA> 
    ## 2 1947  February      23      51     253      23      45     198 dem       <NA> 
    ## 3 1947  March         23      51     253      23      45     198 dem       <NA> 
    ## 4 1947  April         23      51     253      23      45     198 dem       <NA> 
    ## 5 1947  May           23      51     253      23      45     198 dem       <NA> 
    ## # ℹ 817 more rows
    ## # ℹ 2 more variables: close <dbl>, percentage_of_unemployment <dbl>

Since there are no corresponding variables to it in the other two
datasets, we may also remove the `day` variable from `snp_df` before
merging it, depending on the usage of the results, as shown in the code
chunk below:

``` r
snp_df_rm = select(snp_df, -day)

problem1_df_rm = 
  left_join(pols_month_df, snp_df_rm) |>
  left_join(unemployment_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

``` r
problem1_df_rm
```

    ## # A tibble: 822 × 11
    ##   year  month    gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president close
    ##   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>
    ## 1 1947  January       23      51     253      23      45     198 dem          NA
    ## 2 1947  February      23      51     253      23      45     198 dem          NA
    ## 3 1947  March         23      51     253      23      45     198 dem          NA
    ## 4 1947  April         23      51     253      23      45     198 dem          NA
    ## 5 1947  May           23      51     253      23      45     198 dem          NA
    ## # ℹ 817 more rows
    ## # ℹ 1 more variable: percentage_of_unemployment <dbl>

## describe datasets

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

The main data contained in the three datasets are listed as follow:

The file “pols-month” contains 822 observations of 9 variables related
to the number of national politicians who are democratic or republican
at any given time:

- mon: date of the count
- prez_gop: indicator of whether the president was republican on the
  associated date (1 = yes, 0 = no)
- gov_gop: the number of republican governors on the associated date
- sen_gop: the number of republican senators on the associated date
- rep_gop: the number of republican representatives on the associated
  date
- prez_dem: indicator of whether the president was democratic on the
  associated date (1 = yes, 0 = no)
- gov_dem: the number of democratic governors on the associated date
- sen_dem: the number of democratic senators on the associated date
- rep_dem: the number of democratic representatives on the associated
  date

The file “snp” contains 787 observations of 2 variables related to
Standard & Poor’s stock market index (S&P), often used as a
representative measure of stock market as a whole:

- date: the date of the observation
- close: the closing values of the S&P stock index on the associated
  date

The file “unemployment” contains 68 observations of 13 variables:

- Year: the year of the measurements on that row
- Jan: percentage of unemployment in January of the associated year
- Feb: percentage of unemployment in February of the associated year
- Mar: percentage of unemployment in March of the associated year
- Apr: percentage of unemployment in April of the associated year
- May: percentage of unemployment in May of the associated year
- Jun: percentage of unemployment in June of the associated year
- Jul: percentage of unemployment in July of the associated year
- Aug: percentage of unemployment in August of the associated year
- Sep: percentage of unemployment in September of the associated year
- Oct: percentage of unemployment in October of the associated year
- Nov: percentage of unemployment in November of the associated year
- Dec: percentage of unemployment in December of the associated year

Using the code chunk below to further describe the resulting dataset:

``` r
skimr::skim(problem1_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem1_df |
| Number of rows                                   | 822         |
| Number of columns                                | 12          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 4           |
| numeric                                          | 8           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| year          |         0 |          1.00 |   4 |   4 |     0 |       69 |          0 |
| month         |         0 |          1.00 |   3 |   9 |     0 |       12 |          0 |
| president     |         0 |          1.00 |   3 |   3 |     0 |        2 |          0 |
| day           |        36 |          0.96 |   1 |   1 |     0 |        4 |          0 |

**Variable type: numeric**

| skim_variable              | n_missing | complete_rate |   mean |     sd |     p0 |    p25 |    p50 |    p75 |    p100 | hist  |
|:---------------------------|----------:|--------------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|:------|
| gov_gop                    |         0 |          1.00 |  22.48 |   5.68 |  12.00 |  18.00 |  22.00 |  28.00 |   34.00 | ▆▆▇▅▅ |
| sen_gop                    |         0 |          1.00 |  46.10 |   6.38 |  32.00 |  42.00 |  46.00 |  51.00 |   56.00 | ▃▃▇▇▇ |
| rep_gop                    |         0 |          1.00 | 194.92 |  29.24 | 141.00 | 176.00 | 195.00 | 222.00 |  253.00 | ▃▇▆▃▅ |
| gov_dem                    |         0 |          1.00 |  27.20 |   5.94 |  17.00 |  22.00 |  28.00 |  32.00 |   41.00 | ▆▅▇▆▂ |
| sen_dem                    |         0 |          1.00 |  54.41 |   7.37 |  44.00 |  48.00 |  53.00 |  58.00 |   71.00 | ▇▆▇▃▂ |
| rep_dem                    |         0 |          1.00 | 244.97 |  31.37 | 188.00 | 211.00 | 250.00 | 268.00 |  301.00 | ▇▂▇▇▅ |
| close                      |        36 |          0.96 | 472.85 | 543.29 |  17.05 |  83.67 | 137.26 | 932.06 | 2107.39 | ▇▁▂▁▁ |
| percentage_of_unemployment |        12 |          0.99 |   5.83 |   1.65 |   2.50 |   4.70 |   5.60 |   6.90 |   10.80 | ▃▇▅▂▁ |

The merged dataset contains 822 observations of 12 variables.

The variable `year` included in this dataset ranges from 1947 to 2015.

The names of some key variables in the dataset include: year, month,
gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president, day,
close, percentage_of_unemployment.

# Problem 2

## import and tidy Mr. Trash Wheel

Read and clean the Mr. Trash Wheel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  `read_excel`
- use reasonable variable names
- omit rows that do not include dumpster-specific data

``` r
mr_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Mr. Trash Wheel",
                     range = "A2:N587") |>
  janitor::clean_names() |>
  drop_na(dumpster)

mr_wheel_trash_df
```

    ## # A tibble: 584 × 14
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 579 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>

## update Mr. Trash Wheel

The data include a column for the (approximate) number of homes powered.
This calculation is described in the `Homes powered note`, but not
applied to every row in the dataset. Update the data to include a new
`homes_powered` variable based on this calculation.

- Homes Powered - Each ton of trash equates to on average 500 kilowatts
  of electricity. An average household will use 30 kilowatts per day.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df,
                           homes_powered = weight_tons * 500 / 30)

mr_wheel_trash_df
```

    ## # A tibble: 584 × 14
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 579 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>

## tidy and process more data

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to all
datasets before combining.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df, trash_wheel = "mr")

mr_wheel_trash_df
```

    ## # A tibble: 584 × 15
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 579 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
professor_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Professor Trash Wheel",
                     range = "A2:M109") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "professor",
         year = as.character(year))

professor_wheel_trash_df
```

    ## # A tibble: 106 × 14
    ##   dumpster month    year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>    <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 January  2017  2017-01-02 00:00:00        1.79                 15
    ## 2        2 January  2017  2017-01-30 00:00:00        1.58                 15
    ## 3        3 February 2017  2017-02-26 00:00:00        2.32                 18
    ## 4        4 February 2017  2017-02-26 00:00:00        3.72                 15
    ## 5        5 February 2017  2017-02-28 00:00:00        1.45                 15
    ## # ℹ 101 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel <chr>

``` r
gwynnda_df = 
  readxl::read_excel("data/trash_wheel_datasets/202309 Trash Wheel Collection Data.xlsx",
                     sheet = "Gwynnda Trash Wheel",
                     range = "A2:L159") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "gwynnda",
         year = as.character(year))

gwynnda_df
```

    ## # A tibble: 155 × 13
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 July  2021  2021-07-03 00:00:00        0.93                 15
    ## 2        2 July  2021  2021-07-07 00:00:00        2.26                 15
    ## 3        3 July  2021  2021-07-07 00:00:00        1.62                 15
    ## 4        4 July  2021  2021-07-16 00:00:00        1.76                 15
    ## 5        5 July  2021  2021-07-30 00:00:00        1.53                 15
    ## # ℹ 150 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel <chr>

``` r
problem2_df = 
  full_join(mr_wheel_trash_df, professor_wheel_trash_df) |>
  full_join(gwynnda_df)
```

    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## glass_bottles, plastic_bags, wrappers, homes_powered, trash_wheel)`
    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## plastic_bags, wrappers, homes_powered, trash_wheel)`

``` r
problem2_df
```

    ## # A tibble: 845 × 15
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## # ℹ 840 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <dbl>, homes_powered <dbl>, trash_wheel <chr>

## describe the dataset

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in July of 2021?

Mr. Trash Wheel is a water-wheel vessel that removes trash from the
Inner Harbor in Baltimore, Maryland. The combined datasets include the
information related to the date, weight and sort of trash collected by
Mr. , Professor and Gwynnda Trash Wheels.

Apply the code below to take a brief view at the dataset.

``` r
skimr::skim(problem2_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem2_df |
| Number of rows                                   | 845         |
| Number of columns                                | 15          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 3           |
| numeric                                          | 11          |
| POSIXct                                          | 1           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| month         |         0 |             1 |   3 |   9 |     0 |       14 |          0 |
| year          |         0 |             1 |   4 |   4 |     0 |       10 |          0 |
| trash_wheel   |         0 |             1 |   2 |   9 |     0 |        3 |          0 |

**Variable type: numeric**

| skim_variable      | n_missing | complete_rate |     mean |       sd |     p0 |     p25 |     p50 |      p75 |      p100 | hist  |
|:-------------------|----------:|--------------:|---------:|---------:|-------:|--------:|--------:|---------:|----------:|:------|
| dumpster           |         0 |          1.00 |   223.01 |   176.08 |   1.00 |   71.00 |  162.00 |   373.00 |    584.00 | ▇▃▃▃▃ |
| weight_tons        |         0 |          1.00 |     3.01 |     0.81 |   0.61 |    2.49 |    3.07 |     3.54 |      5.62 | ▁▅▇▃▁ |
| volume_cubic_yards |         0 |          1.00 |    15.13 |     1.35 |   5.00 |   15.00 |   15.00 |    15.00 |     20.00 | ▁▁▁▇▁ |
| plastic_bottles    |         1 |          1.00 |  2296.47 |  1715.51 |   0.00 | 1000.00 | 1980.00 |  2900.00 |   9830.00 | ▇▆▁▁▁ |
| polystyrene        |         1 |          1.00 |  1631.29 |  1916.38 |   0.00 |  280.00 |  950.00 |  2400.00 |  11528.00 | ▇▂▁▁▁ |
| cigarette_butts    |         1 |          1.00 | 15592.43 | 25614.82 |   0.00 | 3200.00 | 5500.00 | 16000.00 | 310000.00 | ▇▁▁▁▁ |
| glass_bottles      |       156 |          0.82 |    20.89 |    15.51 |   0.00 |   10.00 |   18.00 |    28.00 |    110.00 | ▇▃▁▁▁ |
| plastic_bags       |         1 |          1.00 |  1081.95 |  1490.05 |   0.00 |  280.00 |  680.00 |  1400.00 |  13450.00 | ▇▁▁▁▁ |
| wrappers           |       118 |          0.86 |  2330.41 |  2892.73 | 180.00 |  840.00 | 1380.00 |  2635.00 |  20100.00 | ▇▁▁▁▁ |
| sports_balls       |       261 |          0.69 |    13.17 |     9.56 |   0.00 |    6.00 |   11.00 |    18.25 |     56.00 | ▇▆▂▁▁ |
| homes_powered      |         0 |          1.00 |    50.16 |    13.56 |  10.17 |   41.50 |   51.17 |    59.00 |     93.67 | ▁▅▇▃▁ |

**Variable type: POSIXct**

| skim_variable | n_missing | complete_rate | min        | max        | median     | n_unique |
|:--------------|----------:|--------------:|:-----------|:-----------|:-----------|---------:|
| date          |         0 |             1 | 1900-01-20 | 2023-06-30 | 2019-10-25 |      457 |

The resulting dataset consists of 845 observations of 15 variables.

The key variables in the dataset include: dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
homes_powered, trash_wheel.

For available data, the total weight of trash collected by Professor
Trash Wheel is 216.26 tons.

And the total number of cigarette butts collected by Gwynnda in July of
2021 is 1.63^{4}.

# Problem 3

## import and tidy baseline data

Import, clean, and tidy the dataset of baseline demographics. Ensure
that sex and APOE4 carrier status are appropriate encoded (i.e. not
numeric), and remove any participants who do not meet the stated
inclusion criteria (i.e. no MCI at baseline). Discuss important steps in
the import process and relevant features of the dataset. How many
participants were recruited, and of these how many develop MCI? What is
the average baseline age? What proportion of women in the study are
APOE4 carriers?

``` r
mci_baseline_df = 
  read_csv("data/data_mci/MCI_baseline.csv", 
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor()),
           na = c(".")
           ) |>
  janitor::clean_names() |>
  filter(current_age <= age_at_onset | is.na(age_at_onset))

mci_baseline_df
```

    ## # A tibble: 480 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl>
    ## 1     1        63.1 0            16 1             NA  
    ## 2     2        65.6 0            20 1             NA  
    ## 3     3        62.5 1            16 1             66.8
    ## 4     4        69.8 0            16 0             NA  
    ## 5     5        66   1            16 0             68.7
    ## # ℹ 475 more rows

The data was collected in an observational study to understand the
trajectory of Alzheimer’s disease (AD) biomarkers. Study participants
were free of Mild Cognitive Impairment (MCI), a stage between the
expected cognitive decline of normal aging and the more serious decline
of dementia, at the study baseline.

Basic demographic information were measured at the study baseline. The
study monitored the development of MCI and recorded the age of MCI onset
during the follow-up period, with the last visit marking the end of
follow-up. APOE4 is a variant of the apolipoprotein E gene,
significantly associated with a higher risk of developing Alzheimer’s
disease.

Some important steps in the process include skipping the first row which
contains no variavle names or values but notes, setting the variable
type of some specific columns, and converting missing values to `NA`
before dropping the rows containing them.

The original dataset consists of 484 observations of 6 variables. Some
key variables include: …1, Age at the study baseline, 1 = Male, 0 =
Female, Years of education, 1 = APOE4 carrier, 0 = APOE4 non-carrier,
Age at the onset of MCI; missing if a subject remains MCI free during
the follow-up period.

After a primary filtering, the resulting dataset is composed by 480 rows
and 6 columns.

It can be concluded that 484 participants were recruited at the
beginning of the study, and 480 of them developed MCI during the track.

Use the following code chunk to calculate the average baseline age and
proportion of APOE4 carriers in women.

``` r
baseline_origin_df = 
  read_csv("data/data_mci/MCI_baseline.csv",
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor())) |>
  janitor::clean_names()
average_age = mean(pull(baseline_origin_df,var = 2))
baseline_women_df = filter(baseline_origin_df, sex == 0)
baseline_women_carrier_df = filter(baseline_women_df, apoe4 == 1)
carrier_proportion = 
  as.numeric((count(baseline_women_carrier_df) / count(baseline_women_df))[1])
```

Thus the average baseline age of the study is 65.0467909 years and
29.8578199% proportion of the women are APOE4 carriers.

``` r
filtered_women_df = filter(mci_baseline_df, sex == 0)
filtered_women_carrier_df = filter(filtered_women_df, apoe4 == 1)
carrier_proportion_filtered = 
  as.numeric((count(filtered_women_carrier_df) / count(filtered_women_df))[1])
```

As shown above, if we exclude the subjects that did not meet the
criteria for baseline study, then the average baseline age will be
65.0320833 years and the proportion of APOE4 carriers in women will be
30%.

## import and process other data

Similarly, import, clean, and tidy the dataset of longitudinally
observed biomarker values; comment on the steps on the import process
and the features of the dataset.

``` r
mci_amyloid_df = 
  read_csv("data/data_mci/mci_amyloid.csv", 
           skip = 1
           ) |>
  janitor::clean_names() |>
  rename(id = study_id)
```

    ## Rows: 487 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Baseline, Time 2, Time 4, Time 6, Time 8
    ## dbl (1): Study ID
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
mci_amyloid_df
```

    ## # A tibble: 487 × 6
    ##      id baseline    time_2      time_4      time_6      time_8     
    ##   <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ## 1     1 0.1105487   <NA>        0.109325197 0.104756131 0.107257697
    ## 2     2 0.107481183 0.109157373 0.109457839 0.105729713 0.10661845 
    ## 3     3 0.106087034 0.108744509 0.106065035 <NA>        0.106152357
    ## 4     4 0.109251358 0.108699686 0.110540386 0.107476797 0.111212209
    ## 5     5 0.107950408 0.112273883 0.115139677 0.106606054 0.106052066
    ## # ℹ 482 more rows

The amyloid β 42/40 ratio holds significant promise for diagnosing and
predicting disease outcomes. This ratio undergoes changes over time and
has been linked to the manifestation of clinical symptoms of Alzheimer’s
disease.

The main step in import process is to skip the first row to avoid notes.
And for future merging of datasets, convert the variable name to the
same.

The dataset contains 487 rows and 6 columns, the main variables of which
include: id, baseline, time_2, time_4, time_6, time_8.

If required, we can also use `pivot_longer` function to further tidy the
dataset into a longer form, the code chunk to achieve which shown as
follow:

``` r
mci_amyloid_longer_df = 
  mci_amyloid_df |> 
    pivot_longer(
    time_2:time_8,
    names_to = "time",
    names_prefix = "time_",
    values_to = "ratio"
  )

mci_amyloid_longer_df
```

    ## # A tibble: 1,948 × 4
    ##      id baseline    time  ratio      
    ##   <dbl> <chr>       <chr> <chr>      
    ## 1     1 0.1105487   2     <NA>       
    ## 2     1 0.1105487   4     0.109325197
    ## 3     1 0.1105487   6     0.104756131
    ## 4     1 0.1105487   8     0.107257697
    ## 5     2 0.107481183 2     0.109157373
    ## # ℹ 1,943 more rows

## compare and join datasets

Check whether some participants appear in only the baseline or amyloid
datasets, and comment on your findings. Combine the demographic and
biomarker datasets so that only participants who appear in both datasets
are retained, and briefly describe the resulting dataset; export the
result as a CSV to your data directory.

Apply `anti_join` function to see the rows contained only in baseline
dataset.

``` r
antijoin1_origin_df = anti_join(baseline_origin_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_origin_df
```

    ## # A tibble: 8 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct> <chr>       
    ## 1    14        58.4 0            20 0     66.2        
    ## 2    49        64.7 1            16 0     68.4        
    ## 3    92        68.6 0            20 0     .           
    ## 4   179        68.1 1            16 0     .           
    ## 5   268        61.4 0            18 1     67.5        
    ## 6   304        63.8 0            16 0     .           
    ## 7   389        59.3 0            16 0     .           
    ## 8   412        67   1            16 1     .

``` r
antijoin1_df = anti_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_df
```

    ## # A tibble: 8 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl>
    ## 1    14        58.4 0            20 0             66.2
    ## 2    49        64.7 1            16 0             68.4
    ## 3    92        68.6 0            20 0             NA  
    ## 4   179        68.1 1            16 0             NA  
    ## 5   268        61.4 0            18 1             67.5
    ## 6   304        63.8 0            16 0             NA  
    ## 7   389        59.3 0            16 0             NA  
    ## 8   412        67   1            16 1             NA

``` r
antijoin1_longer_df = anti_join(baseline_origin_df, mci_amyloid_longer_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_longer_df
```

    ## # A tibble: 8 × 6
    ##      id current_age sex   education apoe4 age_at_onset
    ##   <dbl>       <dbl> <fct>     <dbl> <fct> <chr>       
    ## 1    14        58.4 0            20 0     66.2        
    ## 2    49        64.7 1            16 0     68.4        
    ## 3    92        68.6 0            20 0     .           
    ## 4   179        68.1 1            16 0     .           
    ## 5   268        61.4 0            18 1     67.5        
    ## 6   304        63.8 0            16 0     .           
    ## 7   389        59.3 0            16 0     .           
    ## 8   412        67   1            16 1     .

Similarly, use the code below for the rows contained only in amyloid
dataset.

``` r
antijoin2_origin_df = anti_join(mci_amyloid_df, baseline_origin_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_origin_df
```

    ## # A tibble: 12 × 6
    ##       id baseline    time_2      time_4      time_6      time_8     
    ##    <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ##  1   484 0.11139422  0.110936838 0.109182887 0.110607585 0.107057538
    ##  2   485 0.106042813 0.105158363 0.107758828 0.107281321 0.106181816
    ##  3   486 0.109161071 0.114634379 <NA>        0.110035156 0.107234758
    ##  4   487 0.110821971 0.107791347 0.109855229 0.110951271 0.105861634
    ##  5   488 0.110418756 0.111994328 0.113132987 0.108902038 0.109449907
    ##  6   489 0.11477384  0.113322128 0.115109381 0.116004489 0.112260161
    ##  7   490 0.111762756 0.109627815 0.111492905 0.110104053 <NA>       
    ##  8   491 0.116934974 0.113763228 0.111358448 0.110509854 0.110541984
    ##  9   492 0.109757685 0.109912273 0.110672861 0.109064952 0.109161341
    ## 10   493 0.108357146 0.108161281 0.109491179 0.104448142 0.108636703
    ## 11   494 0.116669151 0.109711076 0.112133216 0.111399722 0.108836759
    ## 12   495 Na          0.105142354 0.108149625 0.105918659 0.102512562

``` r
antijoin2_df = anti_join(mci_amyloid_df, mci_baseline_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_df
```

    ## # A tibble: 15 × 6
    ##       id baseline    time_2      time_4      time_6      time_8     
    ##    <dbl> <chr>       <chr>       <chr>       <chr>       <chr>      
    ##  1    72 0.106965463 <NA>        0.107266218 0.106665207 <NA>       
    ##  2   283 0.113436336 0.106568976 0.11338643  0.10820706  0.114399611
    ##  3   380 0.111158847 0.104560429 0.106822683 0.104961175 0.109506164
    ##  4   484 0.11139422  0.110936838 0.109182887 0.110607585 0.107057538
    ##  5   485 0.106042813 0.105158363 0.107758828 0.107281321 0.106181816
    ##  6   486 0.109161071 0.114634379 <NA>        0.110035156 0.107234758
    ##  7   487 0.110821971 0.107791347 0.109855229 0.110951271 0.105861634
    ##  8   488 0.110418756 0.111994328 0.113132987 0.108902038 0.109449907
    ##  9   489 0.11477384  0.113322128 0.115109381 0.116004489 0.112260161
    ## 10   490 0.111762756 0.109627815 0.111492905 0.110104053 <NA>       
    ## 11   491 0.116934974 0.113763228 0.111358448 0.110509854 0.110541984
    ## 12   492 0.109757685 0.109912273 0.110672861 0.109064952 0.109161341
    ## 13   493 0.108357146 0.108161281 0.109491179 0.104448142 0.108636703
    ## 14   494 0.116669151 0.109711076 0.112133216 0.111399722 0.108836759
    ## 15   495 Na          0.105142354 0.108149625 0.105918659 0.102512562

``` r
antijoin2_longer_df = anti_join(mci_amyloid_longer_df, baseline_origin_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_longer_df
```

    ## # A tibble: 48 × 4
    ##      id baseline    time  ratio      
    ##   <dbl> <chr>       <chr> <chr>      
    ## 1   484 0.11139422  2     0.110936838
    ## 2   484 0.11139422  4     0.109182887
    ## 3   484 0.11139422  6     0.110607585
    ## 4   484 0.11139422  8     0.107057538
    ## 5   485 0.106042813 2     0.105158363
    ## # ℹ 43 more rows

It is obvious that there exist some participants that only appear in the
basline dataset or the amyloid dataset, with both of the comparisons
returning a tibble, no matter the baseline dataset to be used has been
filtered or not.

Comparing the results of `anti_join` function, it can be seen that there
are much more observations that only appear in the amyloid dataset,
especially after filtering the baseline dataset with the inclusion
criteria. When using baseline dataset as the first input for
`anti_join`, the resulting data frames were the same na matter the form
of amyloid dataset was longer or wider. Yet when using amyloid as the
first input, the resulting data frames differed due to the different
forms of amyloid.

Use the function `inner_join` to combine the two datasets so that only
participants who appear in both datasets will be retained. Then export
the result and save it as a csv document.

``` r
innerjoin_df = inner_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
innerjoin_df
```

    ## # A tibble: 472 × 11
    ##      id current_age sex   education apoe4 age_at_onset baseline    time_2 time_4
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl> <chr>       <chr>  <chr> 
    ## 1     1        63.1 0            16 1             NA   0.1105487   <NA>   0.109…
    ## 2     2        65.6 0            20 1             NA   0.107481183 0.109… 0.109…
    ## 3     3        62.5 1            16 1             66.8 0.106087034 0.108… 0.106…
    ## 4     4        69.8 0            16 0             NA   0.109251358 0.108… 0.110…
    ## 5     5        66   1            16 0             68.7 0.107950408 0.112… 0.115…
    ## # ℹ 467 more rows
    ## # ℹ 2 more variables: time_6 <chr>, time_8 <chr>

``` r
write_csv(innerjoin_df, "mci_combined.csv")
```

The resulting dataset consists of 472 observations of 11 variables. The
key variables include id, current_age, sex, education, apoe4,
age_at_onset, baseline, time_2, time_4, time_6, time_8.

Further descriptive information can be obtained using the code chunk
below.

``` r
skimr::skim(innerjoin_df)
```

|                                                  |              |
|:-------------------------------------------------|:-------------|
| Name                                             | innerjoin_df |
| Number of rows                                   | 472          |
| Number of columns                                | 11           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |              |
| Column type frequency:                           |              |
| character                                        | 5            |
| factor                                           | 2            |
| numeric                                          | 4            |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |              |
| Group variables                                  | None         |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| baseline      |         1 |          1.00 |   9 |  11 |     0 |      471 |          0 |
| time_2        |        47 |          0.90 |   2 |  11 |     0 |      424 |          0 |
| time_4        |        39 |          0.92 |   2 |  11 |     0 |      429 |          0 |
| time_6        |        38 |          0.92 |   2 |  11 |     0 |      434 |          0 |
| time_8        |        34 |          0.93 |   2 |  11 |     0 |      437 |          0 |

**Variable type: factor**

| skim_variable | n_missing | complete_rate | ordered | n_unique | top_counts     |
|:--------------|----------:|--------------:|:--------|---------:|:---------------|
| sex           |         0 |             1 | FALSE   |        2 | 1: 267, 0: 205 |
| apoe4         |         0 |             1 | FALSE   |        2 | 0: 330, 1: 142 |

**Variable type: numeric**

| skim_variable | n_missing | complete_rate |   mean |     sd |   p0 |    p25 |   p50 |    p75 |  p100 | hist  |
|:--------------|----------:|--------------:|-------:|-------:|-----:|-------:|------:|-------:|------:|:------|
| id            |         0 |          1.00 | 242.47 | 139.53 |  1.0 | 122.75 | 241.5 | 362.25 | 483.0 | ▇▇▇▇▇ |
| current_age   |         0 |          1.00 |  65.05 |   2.94 | 56.0 |  63.20 |  64.9 |  67.00 |  72.9 | ▁▃▇▆▁ |
| education     |         0 |          1.00 |  16.39 |   2.05 | 12.0 |  16.00 |  16.0 |  18.00 |  20.0 | ▂▁▇▂▂ |
| age_at_onset  |       381 |          0.19 |  70.47 |   3.56 | 61.2 |  68.30 |  70.3 |  73.50 |  77.2 | ▁▃▇▆▅ |

If we use the longer form of amyloid to do the process above:

``` r
innerjoin_longer_df = inner_join(mci_baseline_df, mci_amyloid_longer_df)
```

    ## Joining with `by = join_by(id)`

``` r
innerjoin_longer_df
```

    ## # A tibble: 1,888 × 9
    ##      id current_age sex   education apoe4 age_at_onset baseline    time  ratio  
    ##   <dbl>       <dbl> <fct>     <dbl> <fct>        <dbl> <chr>       <chr> <chr>  
    ## 1     1        63.1 0            16 1               NA 0.1105487   2     <NA>   
    ## 2     1        63.1 0            16 1               NA 0.1105487   4     0.1093…
    ## 3     1        63.1 0            16 1               NA 0.1105487   6     0.1047…
    ## 4     1        63.1 0            16 1               NA 0.1105487   8     0.1072…
    ## 5     2        65.6 0            20 1               NA 0.107481183 2     0.1091…
    ## # ℹ 1,883 more rows

``` r
write_csv(innerjoin_longer_df, "mci_combined_longer.csv")

skimr::skim(innerjoin_longer_df)
```

|                                                  |                     |
|:-------------------------------------------------|:--------------------|
| Name                                             | innerjoin_longer_df |
| Number of rows                                   | 1888                |
| Number of columns                                | 9                   |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |                     |
| Column type frequency:                           |                     |
| character                                        | 3                   |
| factor                                           | 2                   |
| numeric                                          | 4                   |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |                     |
| Group variables                                  | None                |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| baseline      |         4 |          1.00 |   9 |  11 |     0 |      471 |          0 |
| time          |         0 |          1.00 |   1 |   1 |     0 |        4 |          0 |
| ratio         |       158 |          0.92 |   2 |  11 |     0 |     1721 |          0 |

**Variable type: factor**

| skim_variable | n_missing | complete_rate | ordered | n_unique | top_counts      |
|:--------------|----------:|--------------:|:--------|---------:|:----------------|
| sex           |         0 |             1 | FALSE   |        2 | 1: 1068, 0: 820 |
| apoe4         |         0 |             1 | FALSE   |        2 | 0: 1320, 1: 568 |

**Variable type: numeric**

| skim_variable | n_missing | complete_rate |   mean |     sd |   p0 |    p25 |   p50 |    p75 |  p100 | hist  |
|:--------------|----------:|--------------:|-------:|-------:|-----:|-------:|------:|-------:|------:|:------|
| id            |         0 |          1.00 | 242.47 | 139.42 |  1.0 | 122.75 | 241.5 | 362.25 | 483.0 | ▇▇▇▇▇ |
| current_age   |         0 |          1.00 |  65.05 |   2.93 | 56.0 |  63.20 |  64.9 |  67.00 |  72.9 | ▁▃▇▆▁ |
| education     |         0 |          1.00 |  16.39 |   2.05 | 12.0 |  16.00 |  16.0 |  18.00 |  20.0 | ▂▁▇▂▂ |
| age_at_onset  |      1524 |          0.19 |  70.47 |   3.55 | 61.2 |  68.20 |  70.3 |  73.60 |  77.2 | ▁▃▇▆▅ |

The resulting dataset consists of 1888 observations of 9 variables. The
key variables include id, current_age, sex, education, apoe4,
age_at_onset, baseline, time, ratio.
