p8105_hw2_ml5018
================
Luan Mengxiao
2023-09-26

This is a R Markdown document for homework 2.

Load the package to be used for data processing.

``` r
library(tidyverse)
```

# Problem 1

## pols_month

First, clean the data in pols-month.csv. Use `separate()` to break up
the variable mon into integer variables `year`, `month`, and `day`;
replace month number with month name; create a `president` variable
taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; and
remove the `day` variable.

``` r
pols_month_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(month = month.name[as.numeric(month)]) |>
  mutate(president = prez_gop - prez_dem,
         president = case_match(president, 1  ~ "gop", -1 ~ "dem")) |>
  select(-prez_dem, -prez_gop,-day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## snp

Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that `year` and `month` are the leading columns.

``` r
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(month = month.name[as.numeric(month)],
         year = as.numeric(year),
         year = as.character(case_when(year >= 23 ~ year + 1900,
                          year <  23 ~ year + 2000))) |>
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## unemployment

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

``` r
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(jan:dec, 
               names_to = "month", 
               values_to = "percentage_of_unemployment") |>
  mutate(year = as.character(year),
         month = recode(month,
                        jan = "January",
                        feb = "February",
                        mar = "March",
                        apr = "April",
                        may = "May",
                        jun = "June",
                        jul = "July",
                        aug = "August",
                        sep = "September",
                        oct = "October",
                        nov = "November",
                        dec = "December"
                        )
         )
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## join datasets

Join the datasets by merging `snp` into `pols`, and merging
`unemployment` into the result.

``` r
problem1_df = 
  left_join(pols_month_df, snp_df) |>
  left_join(unemployment_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

## describe datasets

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

The main data contained in the three datasets are listed as follow:

The file “pols-month” contains 822 observations of 9 variables related
to the number of national politicians who are democratic or republican
at any given time:

- mon: date of the count
- prez_gop: indicator of whether the president was republican on the
  associated date (1 = yes, 0 = no)
- gov_gop: the number of republican governors on the associated date
- sen_gop: the number of republican senators on the associated date
- rep_gop: the number of republican representatives on the associated
  date
- prez_dem: indicator of whether the president was democratic on the
  associated date (1 = yes, 0 = no)
- gov_dem: the number of democratic governors on the associated date
- sen_dem: the number of democratic senators on the associated date
- rep_dem: the number of democratic representatives on the associated
  date

The file “snp” contains 787 observations of 2 variables related to
Standard & Poor’s stock market index (S&P), often used as a
representative measure of stock market as a whole:

- date: the date of the observation
- close: the closing values of the S&P stock index on the associated
  date

The file “unemployment” contains 68 observations of 13 variables:

- Year: the year of the measurements on that row
- Jan: percentage of unemployment in January of the associated year
- Feb: percentage of unemployment in February of the associated year
- Mar: percentage of unemployment in March of the associated year
- Apr: percentage of unemployment in April of the associated year
- May: percentage of unemployment in May of the associated year
- Jun: percentage of unemployment in June of the associated year
- Jul: percentage of unemployment in July of the associated year
- Aug: percentage of unemployment in August of the associated year
- Sep: percentage of unemployment in September of the associated year
- Oct: percentage of unemployment in October of the associated year
- Nov: percentage of unemployment in November of the associated year
- Dec: percentage of unemployment in December of the associated year

Using the code chunk below to further describe the resulting dataset:

``` r
skimr::skim(problem1_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem1_df |
| Number of rows                                   | 822         |
| Number of columns                                | 12          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 4           |
| numeric                                          | 8           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| year          |         0 |          1.00 |   4 |   4 |     0 |       69 |          0 |
| month         |         0 |          1.00 |   3 |   9 |     0 |       12 |          0 |
| president     |         5 |          0.99 |   3 |   3 |     0 |        2 |          0 |
| day           |        36 |          0.96 |   1 |   1 |     0 |        4 |          0 |

**Variable type: numeric**

| skim_variable              | n_missing | complete_rate |   mean |     sd |     p0 |    p25 |    p50 |    p75 |    p100 | hist  |
|:---------------------------|----------:|--------------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|:------|
| gov_gop                    |         0 |          1.00 |  22.48 |   5.68 |  12.00 |  18.00 |  22.00 |  28.00 |   34.00 | ▆▆▇▅▅ |
| sen_gop                    |         0 |          1.00 |  46.10 |   6.38 |  32.00 |  42.00 |  46.00 |  51.00 |   56.00 | ▃▃▇▇▇ |
| rep_gop                    |         0 |          1.00 | 194.92 |  29.24 | 141.00 | 176.00 | 195.00 | 222.00 |  253.00 | ▃▇▆▃▅ |
| gov_dem                    |         0 |          1.00 |  27.20 |   5.94 |  17.00 |  22.00 |  28.00 |  32.00 |   41.00 | ▆▅▇▆▂ |
| sen_dem                    |         0 |          1.00 |  54.41 |   7.37 |  44.00 |  48.00 |  53.00 |  58.00 |   71.00 | ▇▆▇▃▂ |
| rep_dem                    |         0 |          1.00 | 244.97 |  31.37 | 188.00 | 211.00 | 250.00 | 268.00 |  301.00 | ▇▂▇▇▅ |
| close                      |        36 |          0.96 | 472.85 | 543.29 |  17.05 |  83.67 | 137.26 | 932.06 | 2107.39 | ▇▁▂▁▁ |
| percentage_of_unemployment |        12 |          0.99 |   5.83 |   1.65 |   2.50 |   4.70 |   5.60 |   6.90 |   10.80 | ▃▇▅▂▁ |

The merged dataset contains 822 observations of 12 variables.

The variable `year` included in this dataset ranges from 1947 to 2015.

The names of some key variables in the dataset include: year, month,
gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president, day,
close, percentage_of_unemployment.

# Problem 2

## import and tidy Mr. Trash Wheel

Read and clean the Mr. Trash Wheel sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data

``` r
mr_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Mr. Trash Wheel",
                     range = "A2:N550") |>
  janitor::clean_names() |>
  drop_na(dumpster)
```

## update Mr. Trash Wheel

The data include a column for the (approximate) number of homes powered.
This calculation is described in the `Homes powered note`, but not
applied to every row in the dataset. Update the data to include a new
`homes_powered` variable based on this calculation.

- Homes Powered - Each ton of trash equates to on average 500 kilowatts
  of electricity. An average household will use 30 kilowatts per day.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df,
                           homes_powered = weight_tons * 500 / 30)
```

## tidy and process more data

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash
Wheel dataset to produce a single tidy dataset. To keep track of which
Trash Wheel is which, you may need to add an additional variable to all
datasets before combining.

``` r
mr_wheel_trash_df = mutate(mr_wheel_trash_df, trash_wheel = "mr")

professor_wheel_trash_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Professor Trash Wheel",
                     range = "A2:M97") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "professor",
         year = as.character(year))

gwynnda_df = 
  readxl::read_excel("data/trash_wheel_datasets/Trash Wheel Collection Data.xlsx",
                     sheet = "Gwynnda Trash Wheel",
                     range = "A2:K110") |>
  janitor::clean_names() |>
  drop_na(dumpster) |>
  mutate(homes_powered = weight_tons * 500 / 30,
         trash_wheel = "gwynnda",
         year = as.character(year))

problem2_df = 
  full_join(mr_wheel_trash_df, professor_wheel_trash_df) |>
  full_join(gwynnda_df)
```

    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## glass_bottles, grocery_bags, chip_bags, homes_powered, trash_wheel)`
    ## Joining with `by = join_by(dumpster, month, year, date, weight_tons,
    ## volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
    ## homes_powered, trash_wheel)`

## describe the dataset

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in July of 2021?

Apply the code below to take a brief view at the dataset.

``` r
skimr::skim(problem2_df)
```

|                                                  |             |
|:-------------------------------------------------|:------------|
| Name                                             | problem2_df |
| Number of rows                                   | 747         |
| Number of columns                                | 16          |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |             |
| Column type frequency:                           |             |
| character                                        | 3           |
| numeric                                          | 12          |
| POSIXct                                          | 1           |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |             |
| Group variables                                  | None        |

Data summary

**Variable type: character**

| skim_variable | n_missing | complete_rate | min | max | empty | n_unique | whitespace |
|:--------------|----------:|--------------:|----:|----:|------:|---------:|-----------:|
| month         |         0 |             1 |   3 |   9 |     0 |       14 |          0 |
| year          |         0 |             1 |   4 |   4 |     0 |        9 |          0 |
| trash_wheel   |         0 |             1 |   2 |   9 |     0 |        3 |          0 |

**Variable type: numeric**

| skim_variable      | n_missing | complete_rate |     mean |       sd |     p0 |     p25 |     p50 |      p75 |      p100 | hist  |
|:-------------------|----------:|--------------:|---------:|---------:|-------:|--------:|--------:|---------:|----------:|:------|
| dumpster           |         0 |          1.00 |   214.10 |   168.33 |   1.00 |   62.50 |  174.00 |   360.50 |    547.00 | ▇▃▃▃▃ |
| weight_tons        |         0 |          1.00 |     3.01 |     0.83 |   0.61 |    2.49 |    3.07 |     3.57 |      5.62 | ▁▅▇▃▁ |
| volume_cubic_yards |         0 |          1.00 |    15.17 |     1.40 |   5.00 |   15.00 |   15.00 |    15.00 |     20.00 | ▁▁▁▇▁ |
| plastic_bottles    |         0 |          1.00 |  2290.85 |  1773.09 |   0.00 |  980.00 | 1930.00 |  2900.00 |   9830.00 | ▇▆▁▁▁ |
| polystyrene        |         0 |          1.00 |  1815.69 |  1962.61 |   0.00 |  410.00 | 1120.00 |  2625.00 |  11528.00 | ▇▂▁▁▁ |
| cigarette_butts    |         0 |          1.00 | 17183.95 | 26817.38 |   0.00 | 3450.00 | 6400.00 | 19000.00 | 310000.00 | ▇▁▁▁▁ |
| glass_bottles      |       106 |          0.86 |    20.71 |    15.82 |   0.00 |    9.00 |   18.00 |    28.00 |    110.00 | ▇▃▁▁▁ |
| grocery_bags       |       106 |          0.86 |  1217.66 |  1634.36 |  24.00 |  360.00 |  780.00 |  1480.00 |  13450.00 | ▇▁▁▁▁ |
| chip_bags          |       106 |          0.86 |  2405.54 |  3050.01 | 180.00 |  800.00 | 1340.00 |  2684.00 |  20100.00 | ▇▁▁▁▁ |
| sports_balls       |       200 |          0.73 |    12.58 |     9.27 |   0.00 |    6.00 |   11.00 |    18.00 |     56.00 | ▇▅▂▁▁ |
| homes_powered      |         0 |          1.00 |    50.18 |    13.77 |  10.17 |   41.42 |   51.17 |    59.50 |     93.67 | ▁▅▇▃▁ |
| plastic_bags       |       641 |          0.14 |   977.64 |   869.02 |   0.00 |  242.50 |  810.00 |  1475.00 |   3600.00 | ▇▃▂▁▁ |

**Variable type: POSIXct**

| skim_variable | n_missing | complete_rate | min        | max        | median     | n_unique |
|:--------------|----------:|--------------:|:-----------|:-----------|:-----------|---------:|
| date          |         0 |             1 | 1900-01-20 | 2022-07-29 | 2019-04-18 |      400 |

The resulting dataset consists of 747 observations of 16 variables.

The key variables in the dataset include: dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, grocery_bags, chip_bags, sports_balls,
homes_powered, trash_wheel, plastic_bags.

For available data, the total weight of trash collected by Professor
Trash Wheel is 190.12 tons.

And the total number of cigarette butts collected by Gwynnda in July of
2021 is 1.63^{4}.

# Problem 3

## import and tidy baseline data

Import, clean, and tidy the dataset of baseline demographics. Ensure
that sex and APOE4 carrier status are appropriate encoded (i.e. not
numeric), and remove any participants who do not meet the stated
inclusion criteria (i.e. no MCI at baseline). Discuss important steps in
the import process and relevant features of the dataset. How many
participants were recruited, and of these how many develop MCI? What is
the average baseline age? What proportion of women in the study are
APOE4 carriers?

``` r
mci_baseline_df = 
  read_csv("data/data_mci/MCI_baseline.csv", 
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor()),
           na = c(".")
           ) |>
  janitor::clean_names() |>
  drop_na(age_at_onset)
```

Some important steps in the process include skipping the first row which
contains no variavle names or values but notes, setting the variable
type of some specific columns, and converting missing values to `NA`
before dropping the rows containing them.

The original dataset consists of 484 observations of 6 variables. Some
key variables include: …1, Age at the study baseline, 1 = Male, 0 =
Female, Years of education, 1 = APOE4 carrier, 0 = APOE4 non-carrier,
Age at the onset of MCI; missing if a subject remains MCI free during
the follow-up period.

After a primary filtering, the resulting dataset is composed by 97 rows
and 6 columns.

It can be concluded that 484 participants were recruited at the
beginning of the study, and 97 of them developed MCI during the track.

Use the following code chunk to calculate the average baseline age and
proportion of APOE4 carriers in women.

``` r
baseline_origin_df = 
  read_csv("data/data_mci/MCI_baseline.csv",
           skip = 1, 
           col_types = cols(
             Sex = col_factor(),
             apoe4 = col_factor())) |>
  janitor::clean_names()
average_age = mean(baseline_origin_df$current_age)
baseline_women_df = filter(baseline_origin_df, sex == 0)
baseline_women_carrier_df = filter(baseline_women_df, apoe4 == 1)
carrier_proportion = 
  as.numeric((count(baseline_women_carrier_df) / count(baseline_women_df))[1])
```

Thus the average baseline age of the study is 65.0467909 years and
29.8578199% proportion of the women are APOE4 carriers.

## import and process other data

Similarly, import, clean, and tidy the dataset of longitudinally
observed biomarker values; comment on the steps on the import process
and the features of the dataset.

``` r
mci_amyloid_df = 
  read_csv("data/data_mci/mci_amyloid.csv", 
           skip = 1
           ) |>
  janitor::clean_names() |>
  rename(id = study_id)
```

    ## Rows: 487 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Baseline, Time 2, Time 4, Time 6, Time 8
    ## dbl (1): Study ID
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The main step in import process is to skip the first row to avoid notes.

The dataset contains 487 rows and 6 columns, the main variables of which
include: id, baseline, time_2, time_4, time_6, time_8.

## compare and join datasets

Check whether some participants appear in only the baseline or amyloid
datasets, and comment on your findings. Combine the demographic and
biomarker datasets so that only participants who appear in both datasets
are retained, and briefly describe the resulting dataset; export the
result as a CSV to your data directory.

Apply `anti_join` function to see the rows contained only in baseline
dataset.

``` r
antijoin1_origin_df = anti_join(baseline_origin_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin1_df = anti_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

Similarly, use the code below for the rows contained only in amyloid
dataset.

``` r
antijoin2_origin_df = anti_join(mci_amyloid_df, baseline_origin_df)
```

    ## Joining with `by = join_by(id)`

``` r
antijoin2_df = anti_join(mci_amyloid_df, mci_baseline_df)
```

    ## Joining with `by = join_by(id)`

Comparing the results of `anti_join` function, it can be seen that there
are much more observations that only appear in the amyloid dataset,
especially after filtering the baseline dataset with the inclusion
criteria.

Use the function `inner_join` to combine the two datasets so that only
participants who appear in both datasets will be retained. Then export
the result and save it as a csv document.

``` r
innerjoin_df = inner_join(mci_baseline_df, mci_amyloid_df)
```

    ## Joining with `by = join_by(id)`

``` r
write_csv(innerjoin_df, "mci_combined.csv")
```

The resulting dataset consists of 94 observations of 11 variables. The
key variables include id, current_age, sex, education, apoe4,
age_at_onset, baseline, time_2, time_4, time_6, time_8.
